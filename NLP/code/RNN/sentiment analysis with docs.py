"""
Continue from 2:38 Tutorial
https://www.freecodecamp.org/learn/machine-learning-with-python/tensorflow/natural-language-processing-with-rnns-sentiment-analysis

"""
from keras.datasets import imdb
from keras.preprocessing import sequence
import tensorflow as tf
import os
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

VOCAB_SIZE = 88584
MAXLEN = 250
BATCH_SIZE = 64

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)
before_pad = train_data[0]
len(train_data[0]) # every review either should have unique length
print("train data --> ", train_data[0])
print("train data length ---> ", len(train_data[0]))
# print("train data type --> ", type(train_data[0]))

"""

More PreProcessing
If we have a look at some of our loaded in reviews we'll notice that they are different lengths. This is 
an issue, we cannot pass different length data into our neural network. Therefore we must make each review 
the same length. To do this we will follow the procedure below:
 - If the review  is greater than 250 words then trim off the extra words
 - If the review is less than 250 words add the necessary amount of 0's to make it equal to 250.

Luckily for us keras has a function that can do this for us:

This means we are going to add some kind of padding to our review. So in this case, I believe we're actually 
going to pad to the left side, which means that say we have a review of length, you know 200, we're going to
add 50, just kind of blank words, which will represent with the index 0 to the left side of the review to make
it the necessary length. So that's good we'll do that. 

So if we look at training data and test data, what this does we have imported keras for .pad_sequences
again we're treating our text data as a sequence, as we've talked about, we're gonna say sequence.pad_sequences,
train_data and passing length that we want to pad it to. So that's what this will do it, we'll perform 
these steps that we've already talked about. 

Again we are going to assign test data and train data to you know, whatever this does for us, we'll going to 
assign test data and train data to you know, whatever this does for us, we'll pass the entire thing.
It'll pass all of them for us at once.

Okay, so let's run that. And the let's just have look at say, trained out of one now, because remember,
this was 189, right? So if we look at train data, so train_data, one, like that, we have

"""

train_data = sequence.pad_sequences(train_data, MAXLEN)
after_pad = train_data[0]
# test_data = sequence.pad_sequences(test_data, MAXLEN)
print("train data after padding ", train_data)
print("texting data after padding ", test_data)

# print("train data after padding ", train_data[0]) # compare with original train_data[0] before pad
# print("texting data after padding ", test_data[0]) # compare with original test_data[0] before and after pad

"""
Creating the Model:
Now it's time to create the model. We'll use a word embedding layer as the first layer in our model and
add a LSTM Layer afterwards that feeds into a dense node to get our predicated sentiment.

32 stands for the output dimension of the vectors generated by the embedding layer. We can change this value if we'd like!

From tutorial:

Now this model is pretty straightforward. We have an embedding layer and LSTM and a dense layer here. So the reason we've done 
dense with the activation function of sigmoid at the end, is because we're trying to pretty much predict the Sentiment of 
this right, which means that if we have the sentiment between zero and one, then " if a number is greater than 0.5, we
could classify that as a positive review. And if it's less than 0.5, or equal.
You Know whatever you want to set the bounds that then we can say it's negative review. "

# tf.keras.layers_Dense(1, activation='sigmoid')
So, sigmoid as we probably might recall squished our values between zero and one. So whatever the value is at the end of 
the network will be between zero and one, which means that, you know we can make the accurate prediction.

# tf.keras.layers.Embeddings(VOCAB_SIZE, 32),
Now here, the reason we have Embedding layer like well we've already pre-processed I review is, even though we've preprocessed
this with these integers, and they are a bit more meaningful than just a random lookup table that we've talked about before, we
still want to pass that to an embedding layer, which is going to find a way more meaningful representation for those numbers than 
just their integer values already. So it's going to creat those vectors for us.

# tf.keras.layers.LSTM(32),
And this 32 is denoting the fact we're going to make the output of every single one of our embeddings, or vectors
that are created 32 dimensions, which means that when we pass them to the LSTM layer, we need to tell them LSTM Layer, it's 
going to have 32 dimensions for every single word, which is what we're doing. 

And this will implement that long short term memory process we talked about before, and output the final output to tf keras layers
dense, which will tell us you know, that's what this is right?
It will make the prediction. So that's what this model is, we can see, let me see the model 
summary.

"""
# Magic Happens here
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(VOCAB_SIZE, 32),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(1, activation="sigmoid")
])
model.summary()
print("Model Summary ", model.summary())

import seaborn as sns
import matplotlib.pyplot as plt
# Print length of the sequence before and after padding
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE)
index = 100
before_pad_length = len(train_data[index])
before_pad_values = train_data[index]

train_data = sequence.pad_sequences(train_data, MAXLEN)
after_pad_length = len(train_data[index])
after_pad_values = train_data[index]

# Plotting
plt.figure(figsize=(8, 6))
plt.bar(['Before Padding', 'After Padding'], [before_pad_length, after_pad_length], 
        color=['blue', 'orange'])
plt.title(f'Length of train_data[0] Before and After Padding for train_data[{index}]')
plt.ylabel('Length')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Plotting
plt.figure(figsize=(12, 6))
plt.plot(range(len(before_pad_values)), before_pad_values, label=f'Before Padding ({before_pad_length} words)',
         linestyle='--', color='blue', marker='o')
plt.plot(range(len(after_pad_values)), after_pad_values, label=f'After Padding ({after_pad_length} words)', 
         linestyle='-', color='orange', marker='x')
plt.title(f'Comparison of Values Before and After Padding for train_data[{index}]')
plt.xlabel('Index')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.text(20, 0, 'Before Padding', fontsize=10, verticalalignment='bottom', color='blue')
plt.text(20, 0, 'After Padding', fontsize=10, verticalalignment='top', color='orange')
plt.show()
